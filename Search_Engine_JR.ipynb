{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aad9f9b",
   "metadata": {},
   "source": [
    "# E-Commerce Search Engine Exercise\n",
    "\n",
    "> A minimal, reproducible demo that trains a sentence‑transformer model, stores the embeddings in a FAISS index,\n",
    "> and evaluates retrieval quality with MAP@10 (exact & partial matches).\n",
    "> The Notebook also covers fine-tuning and re-ranking by means of a cross encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db6e133",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1. Imports, Data Loading, preliminary Transformations](#1-imports-data-loading-preliminary-transformations)\n",
    "- [2. Embedding Model and FAISS Vector Store](#2-embedding-model-and-faiss-vector-store)\n",
    "- [3. Helper Functions and first Test](#3-helper-functions-and-first-test)\n",
    "- [4. Compute MAP@10 Score – Only “Exact” matches](#4-compute-map10-score-only-exact-matches)\n",
    "- [5. Compute MAP@10 Score – “Exact” and “Partial” matches](#5-compute-map10-score-exact-and-partial-matches)\n",
    "- [6. Summary and Ideas for Improvement](#6-summary-and-ideas-for-improvement)\n",
    "- [7. Appendix 1: Fine - tuning](#7-appendix-1-fine-tuning)\n",
    "- [8. Appendix 2: Re-ranking](#8-appendix-2-re-ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662f90f",
   "metadata": {},
   "source": [
    "## 1. Imports, Data Loading, preliminary Transformations\n",
    "<a id=\"1-imports-data-loading-preliminary-transformations\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f863af5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query DF:  (480, 3)\n",
      "Product DF:  (42994, 9)\n",
      "Label DF:  (233448, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-03 13:26:49 – Products with non‑empty passages: 42994\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Imports\n",
    "# --------------------------------------------------------------------------- #\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Iterable\n",
    "import logging\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Logging\n",
    "# --------------------------------------------------------------------------- #\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(levelname)s] %(asctime)s – %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Data Loading\n",
    "# --------------------------------------------------------------------------- #\n",
    "# !git clone https://github.com/wayfair/WANDS.git\n",
    "query_df = pd.read_csv(\"WANDS/dataset/query.csv\", sep='\\t')\n",
    "print(\"Query DF: \",query_df.shape)\n",
    "# query_df.head()\n",
    "\n",
    "product_df = pd.read_csv(\"WANDS/dataset/product.csv\", sep='\\t')\n",
    "print(\"Product DF: \",product_df.shape)\n",
    "# product_df.head()\n",
    "\n",
    "# get manually labeled groundtruth lables\n",
    "label_df = pd.read_csv(\"WANDS/dataset/label.csv\", sep='\\t')\n",
    "print(\"Label DF: \",label_df.shape)\n",
    "# label_df.head()\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Build a single passage for each product\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Creation of a combned product name\n",
    "def build_passage(row):\n",
    "    parts = [\n",
    "        str(row[\"product_name\"] or \"\"),\n",
    "        str(row[\"product_description\"] or \"\"),\n",
    "        str(row[\"product_features\"] or \"\"),\n",
    "    ]\n",
    "    # keep only non‑empty, lowercase, collapse spaces\n",
    "    parts = [p.strip().lower() for p in parts if p.strip()]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "product_df[\"passage\"] = product_df.apply(build_passage, axis=1)\n",
    "\n",
    "# Drop products with an empty passages\n",
    "product_df = product_df[product_df[\"passage\"] != \"\"]\n",
    "logger.info(f\"Products with non‑empty passages: {len(product_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd5d6b",
   "metadata": {},
   "source": [
    "## 2. Embedding Model and FAISS Vector Store\n",
    "<a id=\"2-embedding-model-and-faiss-vector-store\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2bd9c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-11-03 13:26:49 – Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 672/672 [06:18<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Embedding model - base model\n",
    "# Converts text into numerical vectors that capture semantic meaning \n",
    "# --------------------------------------------------------------------------- #\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"   #this model is not too big in size and has good performance, 384 dimensions \n",
    "model = SentenceTransformer(MODEL_NAME, device=\"cpu\")   \n",
    "\n",
    "# # --------------------------------------------------------------------------- #\n",
    "# # Uncomment only if you want to work with **fine-tuned model**\n",
    "# # In that case , comment out base model above\n",
    "# # --------------------------------------------------------------------------- #\n",
    "# from torch.utils.data import DataLoader\n",
    "# from sentence_transformers import SentenceTransformer, losses\n",
    "# MODEL_NAME = \"fine_tuned_wands_model3\"\n",
    "# model = SentenceTransformer(MODEL_NAME, device=\"cpu\")\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Encode passages\n",
    "# --------------------------------------------------------------------------- #\n",
    "product_ids = product_df[\"product_id\"].tolist()\n",
    "passages    = product_df[\"passage\"].tolist()\n",
    "\n",
    "embeddings = model.encode(\n",
    "    passages,\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00b25883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Build FAISS index\n",
    "# FAISS Stores a large collection of already‑computed vectors and allows fast similarity queries. Vector size should be the same, \"Flat\" each vector stored as-is (no paritioning)\n",
    "# --------------------------------------------------------------------------- #\n",
    "dim = embeddings.shape[1] # The dimensionality of each vector, created by the embedding model , embeddings.shape[0] would be number of documents\n",
    "index = faiss.IndexFlatIP(dim)  # Inner product ≈ cosine after normalisation; index will return results ranked by cosine similarity\n",
    "\n",
    "# Add all vectors – FAISS expects a numpy array of shape (N, dim)\n",
    "index.add(np.vstack(embeddings).astype(\"float32\"))\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Mapping from FAISS id to product_id\n",
    "# --------------------------------------------------------------------------- #\n",
    "faiss_id_to_pid = {idx: pid for idx, pid in enumerate(product_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f2770",
   "metadata": {},
   "source": [
    "## 3. Helper Functions and first Test\n",
    "<a id=\"3-helper-functions-and-first-test\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ff75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.11it/s]\n",
      "[INFO] 2025-11-03 13:33:12 – Top 5 results for query: 'armchair'\n",
      "[INFO] 2025-11-03 13:33:12 –   24318 – gail 29.5 '' wide armchair\n",
      "[INFO] 2025-11-03 13:33:12 –   32917 – 41 '' wide armchair\n",
      "[INFO] 2025-11-03 13:33:12 –   28200 – wilmont 20.08 '' wide armchair\n",
      "[INFO] 2025-11-03 13:33:12 –   22900 – alison 32.3 '' wide armchair\n",
      "[INFO] 2025-11-03 13:33:12 –   1140 – charnley 47 '' wide chenille armchair\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top products for 'armchair':\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Query helper\n",
    "# Find similar Products: !!!IMPORTANT!!!!\n",
    "# This function is heavily used later. Similarity based on embeddings.\n",
    "# --------------------------------------------------------------------------- #\n",
    "def search_query(query_text: str, k: int = 10):\n",
    "    \"\"\"\n",
    "    Return the top‑k product_ids for a given query string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_text : str\n",
    "        The raw query supplied by the user.\n",
    "    k : int, optional (default=10)\n",
    "        How many results to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        Ranked list of product_ids.\n",
    "    \"\"\"\n",
    "    query_vec = model.encode([query_text], normalize_embeddings=True)\n",
    "    distances, indices = index.search(query_vec, k)   # shape (1, k)\n",
    "    ids = [faiss_id_to_pid[idx] for idx in indices[0]]\n",
    "    return ids\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Quick sanity‑check\n",
    "# --------------------------------------------------------------------------- #\n",
    "test_query = \"armchair\"\n",
    "top_ids = search_query(test_query,k=5)\n",
    "logger.info(f\"Top {len(top_ids)} results for query: '{test_query}'\")\n",
    "print(f\"Top products for '{query_df}':\")\n",
    "for pid in top_ids:\n",
    "    name = product_df.loc[product_df[\"product_id\"] == pid, \"product_name\"].values[0]\n",
    "    logger.info(f\"  {pid} – {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea162de",
   "metadata": {},
   "source": [
    "-> Search_query seems to work, as results returned make sense. See \"Ideas for Improvement\" for more thoughts on better queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72564b8",
   "metadata": {},
   "source": [
    "## 4. Compute MAP@10 Score – Only “Exact” matches\n",
    "<a id=\"4-compute-map10-score-only-exact-matches\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3e242ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# MAP@K calculation\n",
    "# --------------------------------------------------------------------------- #\n",
    "def map_at_k(true_ids, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision at K (MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "    true_ids (list): List of relevant product IDs.\n",
    "    predicted_ids (list): List of predicted product IDs.\n",
    "    k (int): Number of top elements to consider.\n",
    "    Returns:\n",
    "    float: MAP@K score.\n",
    "    \"\"\"\n",
    "    #if either list is empty, return 0\n",
    "    if not len(true_ids) or not len(predicted_ids):\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p_id in enumerate(predicted_ids[:k]):\n",
    "        if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(true_ids), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f98f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 217.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 221.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 232.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 218.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 211.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 228.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 221.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 208.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 113.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 219.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 219.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 170.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 204.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 123.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 213.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 170.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 199.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 149.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 232.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 215.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 97.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 170.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 224.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 219.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 183.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 226.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 220.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 205.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 206.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 158.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 170.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 216.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 95.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 160.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 185.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 229.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 230.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 220.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 202.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 222.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 219.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 155.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 189.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 213.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 193.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 220.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 216.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 228.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 201.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 181.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 60.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 83.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 172.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 166.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 144.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 207.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 141.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 152.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 148.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 135.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 147.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 210.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 174.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 191.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 169.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 180.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 157.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 154.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 150.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 163.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 162.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 164.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 198.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 188.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 151.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 175.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 168.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 153.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 197.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 116.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 61.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 98.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 167.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 187.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 220.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 177.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 184.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 195.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 179.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 178.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 203.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 159.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 182.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 165.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 212.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 176.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 171.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 186.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 196.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 190.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 223.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 161.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 173.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 192.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 194.10it/s]\n",
      "[INFO] 2025-11-03 13:33:16 – MAP@10 (exact matches only) = 0.3404\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Exact‑match helper\n",
    "# --------------------------------------------------------------------------- #\n",
    "grouped_label_df = label_df.groupby('query_id') #created grouped object for further exact matches calculation on query basis\n",
    "\n",
    "# grouped_label_df = label_df_eval.groupby('query_id')  #only uncomment after **fine-tuning** instead line above  \n",
    "\n",
    "def get_exact_matches_for_query(query_id): #implementing a function to retrieve exact match product IDs for a query_id\n",
    "    query_group = grouped_label_df.get_group(query_id)\n",
    "    exact_matches = query_group.loc[query_group['label'] == 'Exact']['product_id'].values\n",
    "    return exact_matches\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Add predictions & relevance lists to query_df\n",
    "# --------------------------------------------------------------------------- #\n",
    "query_df['top_product_ids'] = query_df['query'].apply(search_query) #applying the function to obtain top product IDs and adding top K product IDs to the dataframe \n",
    "query_df['relevant_ids'] = query_df['query_id'].apply(get_exact_matches_for_query) #adding the list of exact match product_IDs from labels_df\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Compute MAP@10 per query\n",
    "# --------------------------------------------------------------------------- #\n",
    "query_df['map@k'] = query_df.apply(lambda x: map_at_k(x['relevant_ids'], x['top_product_ids'], k=10), axis=1)\n",
    "# query_df.head()\n",
    "overall_map10 = query_df[\"map@k\"].mean()   # calculate the MAP across the entire query set\n",
    "logger.info(f\"MAP@10 (exact matches only) = {overall_map10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3982947",
   "metadata": {},
   "source": [
    "## 5. Compute MAP@10 Score - \"Exact\" and \"Partial\" matches\n",
    "<a id=\"5-compute-map10-score-exact-and-partial-matches\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e130abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Pre‑processing helpers\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "query_df_weighted = query_df.copy()\n",
    "\n",
    "def get_relevant_weights(query_id: int):\n",
    "    \"\"\"\n",
    "    Return a dictionary {product_id: weight} for all relevant products\n",
    "    of a given query.  Weight is 1.0 for 'Exact', 0.5 for 'Partial'.\n",
    "    \"\"\"\n",
    "    grp = grouped_label_df.get_group(query_id)\n",
    "    # keep only Exact/Partial labels\n",
    "    rel = grp[grp['label'].isin(['Exact', 'Partial'])]\n",
    "    # convert to dict: product_id -> weight\n",
    "    return dict(zip(rel['product_id'], rel['label'].map({'Exact': 1.0,\n",
    "                                                         'Partial': 0.5})))\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Weighted MAP@K\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "def weighted_map_at_k(relevant_weights, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    weighted_map_at_k(relevant_weights, predicted_ids, k=10)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevant_weights : dict\n",
    "        product_id → relevance weight (1.0 for Exact, 0.5 for Partial)\n",
    "    predicted_ids   : list[int]\n",
    "        list of product ids in ranked order\n",
    "    k               : int\n",
    "        number of top results to consider (default 10)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        weighted MAP@k for this single query\n",
    "    \"\"\"\n",
    "    if not relevant_weights or not predicted_ids:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    hit_count = 0.0          # cumulative weighted hits up to i\n",
    "    considered = set()       # to avoid counting the same product twice\n",
    "\n",
    "    for i, pid in enumerate(predicted_ids[:k], start=1):\n",
    "        if pid in relevant_weights and pid not in considered:\n",
    "            weight = relevant_weights[pid]\n",
    "            hit_count += weight\n",
    "            # precision up to position i uses *cumulative weighted* hits\n",
    "            precision_at_i = hit_count / i\n",
    "            score += precision_at_i * weight   # weight appears twice:\n",
    "            # 1. as part of precision_at_i\n",
    "            # 2. as a multiplicative factor to make exact 1.0, partial 0.5\n",
    "            considered.add(pid)\n",
    "\n",
    "    # Normalise by the total available relevance weight for this query\n",
    "    # (min(#relevant_items, k) would be the unweighted version).\n",
    "    max_relevant_weight = min(sum(relevant_weights.values()), k)  # 1.0×#Exact + 0.5×#Partial\n",
    "    if max_relevant_weight == 0:\n",
    "        return 0.0\n",
    "    return score / max_relevant_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33c858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted MAP@10 : 0.4235\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Build mapping: query_id → {product_id: weight}\n",
    "# --------------------------------------------------------------------------- #\n",
    "query_to_weights = {qid: get_relevant_weights(qid) for qid in label_df['query_id'].unique()}\n",
    "# query_to_weights = {qid: get_relevant_weights(qid) for qid in label_df_eval['query_id'].unique()}  #only uncomment after **fine-tuning** instead line above  \n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Evaluate each query\n",
    "# --------------------------------------------------------------------------- #\n",
    "def eval_query(row):\n",
    "    qid          = row['query_id']\n",
    "    pred_ids     = row['top_product_ids']           # ranked list: !!!IMPORTANT!!! this was generated using search_query, based on embeddings and vector store!\n",
    "    rel_weights  = query_to_weights.get(qid, {})\n",
    "    return weighted_map_at_k(rel_weights, pred_ids, k=10)\n",
    "\n",
    "query_df_weighted ['map@k_weighted'] = query_df_weighted.apply(eval_query, axis=1)\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Final mean over all queries\n",
    "# --------------------------------------------------------------------------- #\n",
    "mean_weighted_map10 = query_df_weighted['map@k_weighted'].mean()\n",
    "logger.info(f\"Weighted MAP@10 = {mean_weighted_map10:.4f}\")\n",
    "# query_df_weighted.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54752a0",
   "metadata": {},
   "source": [
    "## 6. Summary and Ideas for Improvement\n",
    "<a id=\"6-summary-and-ideas-for-improvement\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c8499",
   "metadata": {},
   "source": [
    "Summary:\n",
    "<ul type=\"disc\">\n",
    "  <li> Changes for improvement of MAP score:</li>\n",
    "</ul>\n",
    "<p style=\"margin-left: 2rem;\">- Including an embedding model and vector store improves the performance (MAP-Score: 0.34) over Tf-IDF as it caputes the semantic meaning of text instead of only relying on frequencies of words</p>\n",
    "<p style=\"margin-left: 2rem;\">- Including the partial matches increases the MAP score to 0.40 but the partial match value of 0.5 is random for each query. A LLM could be used to get a more precise value (or re-ranking by means of cross-encoder, see discussion below). Variation in MAP-Score through different partial match values is reasonable: Partial Score 0.1 -> MAP-Score 0.286, artial Score 0.9 -> MAP-Score 0.63</p> \n",
    "<p style=\"margin-left: 2rem;\">- Summarizing query and query_class and all product features leads to worse MAP-Scores: Exact: 0.31, Exact+Partial: 0.38, only summarizing all product features leads to similar MAP-scores as the base version: Exact: 0.35 , Exact+Partial:0.39</p>\n",
    "\n",
    "<ul type=\"disc\">\n",
    "  <li> Ideas for improvement:</li>\n",
    "</ul>\n",
    "<p style=\"margin-left: 2rem;\">- Allow queries that are whole sentence, this allows for more context to be matched with product features and descriptions: e.g. \"I would like to find an armchair that is cushioned well and warm for the winter\"</p>\n",
    "<p style=\"margin-left: 2rem;\">- Fine-tune the embedding model as it was trained on geenric web-corpus and not domain-specific vacabulary such as product names, features, or query style</p>    \n",
    "<p style=\"margin-left: 2rem;\">- Apply re-ranking of results using a cross-encoder. Cosine-similarity ranking only considers vector distance. Cross-encoder can score query-passage pairs with a full\n",
    "attention over both texts, capturing subtle matching cues. A lightweight way to fix that is to take the top‑N candidates from FAISS (e.g., N=50–200) and re‑score them with a cross‑encoder that looks at the joint query‑passage representation.</p>\n",
    "\n",
    "\n",
    "Remark:\n",
    "- this notebook utilized code assistants and the provided notebook code, but overall code&logic and structure as well as code customizations were created by owner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7386d0f",
   "metadata": {},
   "source": [
    "## 7. Appendix 1: Fine - tuning\n",
    "<a id=\"7-appendix-1-fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a65ef",
   "metadata": {},
   "source": [
    "In the following, samples are taking from the original datset to conduct fine-tuning using the same embedded model as above. This makes the embeddings aware of the WAND dataset context. The samples are then removed from the original dataset before the code above (after imports and data loading) can be re-run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dccaf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 5000 pairs for fine‑tuning.\n",
      "Evaluation label_df size reduced from 233448 to 228378\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# Imports\n",
    "# --------------------------------------------------------------------------- #\n",
    "import random\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "# Feature-Value Pairs (Training Samples)\n",
    "# --------------------------------------------------------------------------- #\n",
    "train_pairs = []\n",
    "for _, row in label_df.iterrows():\n",
    "    query_text = query_df.loc[query_df['query_id'] == row['query_id'], 'query'].values[0]\n",
    "    product_text = product_df.loc[product_df['product_id'] == row['product_id'], 'passage'].values[0]\n",
    "    label_value = 1.0 if row['label'] == 'Exact' else 0.5 if row['label'] == 'Partial' else 0.0\n",
    "    train_pairs.append((row['query_id'], row['product_id'], label_value))\n",
    "\n",
    "N_SUBSAMPLE = 5000  #Because laptop heat up, and time considerations, only a few samples are created\n",
    "if len(train_pairs) > N_SUBSAMPLE:\n",
    "    sampled_train_pairs = random.sample(train_pairs, N_SUBSAMPLE)\n",
    "else:\n",
    "    sampled_train_pairs = train_pairs\n",
    "\n",
    "print(f\"Using {len(sampled_train_pairs)} pairs for fine‑tuning.\")\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(\n",
    "        texts=[\n",
    "            query_df.loc[query_df['query_id'] == qid, 'query'].values[0],\n",
    "            product_df.loc[product_df['product_id'] == pid, 'passage'].values[0],\n",
    "        ],\n",
    "        label=lbl\n",
    "    )\n",
    "    for qid, pid, lbl in sampled_train_pairs\n",
    "]\n",
    "\n",
    "# Identify which query_id–product_id pairs were used for training\n",
    "train_query_ids  = [q for q, p, _ in sampled_train_pairs]\n",
    "train_product_ids = [p for q, p, _ in sampled_train_pairs]\n",
    "\n",
    "# Remove those pairs from label_df\n",
    "mask = ~label_df.apply(lambda r: (r['query_id'], r['product_id']) in \n",
    "                       {(q, p) for q, p, _ in sampled_train_pairs}, axis=1)\n",
    "label_df_eval = label_df[mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Evaluation label_df size reduced from {len(label_df)} to {len(label_df_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de11b2",
   "metadata": {},
   "source": [
    "\"label_df_eval\"  - This Dataframe has then to be inserted above at two locations (CTRL+F to search for these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------- #\n",
    "# More Imports and Instantiation of fine-tuning. model\n",
    "# --------------------------------------------------------------------------- #\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992debbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [02:04<00:00,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 124.4874, 'train_samples_per_second': 80.329, 'train_steps_per_second': 2.522, 'train_loss': 1.4076224163079718, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fitting the model, with 5000 samples it only runs around 2 minutes, laptop heat up after ca. 5-6 min\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=2,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "#save the model under a new name, this is saved within the same folder. This name is referenced in Chapter 2 when the fine-tuned model is used.\n",
    "model.save(\"fine_tuned_wands_model3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2902c",
   "metadata": {},
   "source": [
    "Result: \n",
    "- Exact MAP@10: 0.34 (same)\n",
    "- Exact+Partial MAP@10: 0.4235 (increase)\n",
    "An increased value makes sense because the model was fine-tuned on Exact+Partial. With only 5000 samples an improvement of 5.8% can be achieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041d3c1",
   "metadata": {},
   "source": [
    "## 8. Appendix 2: Re-ranking\n",
    "<a id=\"8-appendix-1-re-ranking\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cac61f",
   "metadata": {},
   "source": [
    "Re-ranking by using a CrossEncoder is common among machine learning tasks and often increases accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b7799f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "cross = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd0f7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_query(query_text, top_k=10, n_candidates=50):\n",
    "    # 1. Fast FAISS pass (top-n_candidates)\n",
    "    query_vec = model.encode([query_text], normalize_embeddings=True)\n",
    "    _, indices = index.search(query_vec, n_candidates)\n",
    "    candidate_ids = [faiss_id_to_pid[idx] for idx in indices[0]]\n",
    "    candidate_passages = [product_df.loc[product_df['product_id']==pid, 'passage'].values[0]\n",
    "    for pid in candidate_ids]\n",
    "    # 2. Cross‑encoder scoring\n",
    "    pairs = [[query_text, passage] for passage in candidate_passages]\n",
    "    scores = cross.predict(pairs) # higher = better match\n",
    "    # 3. Sort and keep top_k\n",
    "    ranked = sorted(zip(candidate_ids, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [pid for pid, _ in ranked[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6252b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reranking queries: 100%|██████████| 200/200 [03:18<00:00,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@10 after re‑rank (sample of 200 queries): 0.3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Random sub-samples are used for demonstration purposes only. Will work with total query_df as well. \n",
    "N_SAMPLES = 200\n",
    "sampled_idx = random.sample(list(query_df.index), N_SAMPLES)\n",
    "sampled_df = query_df.loc[sampled_idx].copy()\n",
    "\n",
    "tqdm.pandas(desc=\"Reranking queries\")\n",
    "\n",
    "# Only 25 candidates are chose for re-ranking. This means 25 matches are taken and re-ranked, kepping the 10 needed for MAP@10 score.\n",
    "sampled_df['top_product_ids_cr'] = sampled_df['query'].progress_apply(\n",
    "    lambda q: rerank_query(q, top_k=10, n_candidates=25))\n",
    "\n",
    "sampled_df['map@k_cr'] = sampled_df.apply(\n",
    "    lambda row: map_at_k(row['relevant_ids'], row['top_product_ids_cr'], k=10),\n",
    "    axis=1)\n",
    "print(f\"MAP@10 after re‑rank (sample of {N_SAMPLES} queries): {sampled_df['map@k_cr'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4221c96",
   "metadata": {},
   "source": [
    "Result:\n",
    "- Re-ranking was only applied to the Exact Map@10 score due to time constraints.\n",
    "- Accuracy improved a lot from 0.34 -->0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
